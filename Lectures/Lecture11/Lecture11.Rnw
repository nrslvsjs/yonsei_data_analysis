\documentclass{beamer}
\usetheme{Stats}
\setbeamercovered{transparent}
\usepackage{color}
\usepackage{hyperref}
  \hypersetup{
  	colorlinks=true
		linkcolor=black
		}
\usepackage{url}
\usepackage{graphics}
\usepackage{tikz}
\usepackage{booktabs}

\DeclareMathOperator*{\argmin}{arg\,min}

<<ParentGlobalOpts, echo=FALSE>>=
  options(width=50)
  opts_chunk$set(fig.align='center')
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title Slide %%%%%%%%%%%%%%%%%%%%%%%%%%
\title[]{Intro to Social Science Data Analysis \\[1cm] Week 11 Lecture: Simple Linear Regression}
\author[]{
    \href{mailto:gandrud@yonsei.ac.kr}{Christopher Gandrud}
}
\date{\today}


\begin{document}

\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents}

\frame{
  \frametitle{Assignment 3}
  {\Large{Like all data assignments in this course, your response to Assignment 3 must be \textbf{reproducible}.}}
}

\section{Recap}
\frame{
	\frametitle{Quick Quiz 1}
  Find the sample proportions of the following party's supporters:
  \begin{table}
    \begin{tabular}{c c c c}
    \hline
    Saenuri & DUP & Other & Total \\
    \hline\hline
    1064 & 891 & 520 & 2475 \\ 
    \hline
    \end{tabular}
  \end{table}
}

\frame{
  \frametitle{Quick Quiz 1}
  \begin{table}
    \begin{tabular}{c c c c}
    \hline
    Saenuri & DUP & Other & Total \\
    \hline\hline
    1064 & 891 & 520 & 2475 \\ 
    (0.43) & (0.36) & (0.21) & (1) \\
    \hline
    \end{tabular}
  \end{table}
}

\frame{
  \frametitle{Quick Quiz 2}
  If we wanted to make inferences about {\bf{population proportions}} from sampling proportions, what {\bf{distribution}} do we often assume the sampling proportions follow? \\[0.5cm]
  
  What are it's {\bf{parameters}}?
}

\frame{
  \frametitle{Quick Quix 3}
  Imagine we have a two-way contingency table.
  \begin{table}
    \begin{tabular}{c c c c}
    \hline
     & Attend University & No University \\
    \hline\hline
    Married &&\\ 
    Not Married && \\
    \hline
    \end{tabular}
  \end{table}
  If we conducted a $\chi^{2}$ test with this data and found a p-value of $<0.001$ what would we conclude?
}

\frame{
  \frametitle{Quick Quiz 4}
  Write the simple linear regression equation for how a person's height is related to their income.
}

\frame{
  \frametitle{Quick Quiz 5}
  Describe how a linear regression line would look if the relationship between two variables was negative. \\[0.5cm]
  How would it look if the relationship was positive? \\[0.5cm]
  What about no relationship?
}

\frame{
  \frametitle{Quick Quiz 6}
  Describe the variables in the simple linear regression equation.
  \[
  y = \alpha + \beta x
  \]
}

%%%%%%%%%%%% Correlation
\section{Correlation}
\frame{
  \framtitle{Motivation}
  Since almost no interesting relationship is perfectly linear, how do we find the {\bf{best fit line}} that describes the relationship between some $x$ and some $y$?
}

\frame{
  \frametitle{How?}
  \begin{center}
    \includegraphics[scale=0.5]{/git_repositories/Introduction_to_Statistics_and_Data_Analysis_Yonsei/Lectures/Lecture10/ExampleSLR.png} \\[0.5cm]
  \end{center}  
{\tiny{Source: Diaz et. al. (2011, 216)}}
}

\frame{
  In {\bf{simple linear regression}} we are trying to find the straight line that is {\bf{as close to all of the data points as possible}}.\\[1cm]
  How do we find this line?
}

\begin{frame}[fragile]
  Let's use the SAT/GPA data from the \texttt{openintro} package:
<<LoadData, message=FALSE>>=
# Load library
library(openintro)

# Load data
data(satGPA)

# Show variables
names(satGPA)

# Subset to remove the sex variable
satGPASlim <- satGPA[, 2:6]
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Plot the SAT Scores \& GPAs}
<<DescribeGPA, fig.height=4, message=FALSE, warning=FALSE, error=FALSE>>=
library(GGally)

ggpairs(satGPASlim, upper = "blank")
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{First Year GPA}
  Universities want to know how well student's total SAT scores (\texttt{SATSum}) relate to their academic performance in the first year of university (\texttt{FYGPA}).
<<SATFYGPA, echo=FALSE, warning=FALSE, fig.height=4>>=
# Load library
library(ggplot2)

#Create plot of SATSum FYGPA 
ggplot(data = satGPASlim, aes(SATSum, FYGPA)) +
        geom_point(colour = "grey60") +
        theme_bw(base_size = 15)
@
\end{frame}

\frame{
  \frametitle{Correlation}
  One way to describe the overall relationship between \texttt{SATSum} and \texttt{FYGPA} is to find the {\bf{correlation}} between the two variables. 
}

\frame{
  \frametitle{Correlation}
{\LARGE{Correlation ($R$):}} \\[0.5cm]
  Describes the {\bf{strength}} of a linear relationship. \\[0.25cm]
  It ranges from -1 to 1. \\[0.5cm]
  -1 indicates a {\bf{perfect negative relationship}}.\\[0.25cm]
  1 indicates a {\bf{perfect positive relationship}}. \\[0.25cm]
  0 indicates {\bf{no correlation/relationship}}.  
}

\frame{
  \frametitle{Correlation}
  To find the correlation for observations $(x_{1}, y_{1}),\:(x_{2}, y_{2})\ldots(x_{n}, y_{n})$
\[
  R = \frac{1}{n - 1}\sum\limits_{i = 1}^n\frac{x_{i} - \bar{x}}{s_{x}}\frac{y_{i} - \bar{y}}{s_{y}}
\] 
}

\begin{frame}[fragile]
  \frametitle{Or\ldots}
  Or we can have R do the maths for us.
<<Corr>>=
cor(satGPA$SATSum, satGPA$FYGPA)
@
\end{frame}

\frame{
  \frametitle{Statistical Significance \& Correlation}
  If we wanted to test to see if the correlation is statistically significant, what would the null hypothesis be?
}

\frame{
  \frametitle{Statistical Significance \& Correlation}
{\large{$H_{0}$: $R = 0$ \\[0.5cm]
$H_{a}$: R \neq 0$}} 
}

\begin{frame}[fragile]
  \frametitle{Hypothesis Testing Correlation Coefficients in R}
{\scriptsize{
<<CorrTest>>=
cor.test(satGPA$SATSum, satGPA$FYGPA)
@
}}
\end{frame}

\frame{
  \frametitle{More Correlation Examples}
  \begin{center}
  \includegraphics[scale=0.45]{LinearCor.png} \\[0.5cm]
  \end{center}
{\tiny{Source: Diaz et al. (2011, 282 )}}
}

\frame{
  \frametitle{Caution}
  A low linear correlation {\bf{does not necessarily}} mean a weak relationship. \\[0.5cm]
  It means a weak {\bf{linear}} relationship.
  \begin{center}
  \includegraphics[scale=0.45]{NonLinearCor.png} \\[0.5cm]
  \end{center}
{\tiny{Source: Diaz et al. (2011, 282 )}}
}

%%%%%%%%%%%% Least Squares Regression
\section{Best Fit Lines \& Least Squares Regression}
\frame{
  \frametitle{Best Fit Lines \& Least Squares Regression}
  Ok, linear correlations are useful for finding:
  \begin{itemize}
    \item the {\bf{direction}} of a linear relationship,
    \item the {\bf{strength}} of a linear relationship.
  \end{itemize}
}

\frame{
  \frametitle{More specific}
  What if we want to be more specific? \\[0.5cm]
  For example, using a student's total SAT score to predict their first year university GPA.\\[0.5cm]
{\bf{Note:}} the estimated value of the dependent variable ($y$) is often written $\hat{y}$ ({\emph{``y hat"}}).

}

\begin{frame}[fragile]
  \frametitle{The Linear Best Fit Line}
  The blue line is the closest straight line (``best fit") to all of the data points.
<<BestFit, fig.height=4, echo=FALSE>>=
#Create plot of SATSum FYGPA 
ggplot(data = satGPASlim, aes(SATSum, FYGPA)) +
        geom_point(colour = "grey60") +
        stat_smooth(method="lm", se = FALSE) +
        theme_bw(base_size = 15)
@
\end{frame}

\frame{
  \frametitle{How?}
  \begin{center}
    {\LARGE{How do we find the best fit line?}}
  \end{center}
}

\frame{
  \frametitle{Residuals}
  Well, the best fit line would do something like have the smallest {\bf{residuals}} possible. \\[0.5cm]
  What is a residual?
}

\frame{
  \frametitle{Residuals}
{\LARGE{Residual:}} \\[0.5cm]
  the difference between the observed and expected values based on the best fit model.\\[0.5cm]
  More formally: the residual ($e_{i}$) of the observation $(x_{i}, y_{i})$ is the difference between the observed value of $y_{i}$ and the expected value $\hat{y}_{i}$:
  \[
  e_{i} = y_{i} - \hat{y}_{i}
  \]
}

\begin{frame}[fragile]
  \frametitle{Residuals}
  The red point is at (81, 0.77). Given that \texttt{SATSum} is 81, it is expected to be at 1.935. So, it's residual is $0.77 - 1.935 = -1.65$.
<<Res1, fig.height=4, echo=FALSE>>=
# Create Low Point
satGPASlim$LowPoint <- satGPASlim$FYGPA == 0.77 & satGPASlim$SATSum == 81

# Point Colours
PColour <- c("Grey90", "red")

#Create plot of SATSum FYGPA 
ggplot(data = satGPASlim, aes(SATSum, FYGPA, color = LowPoint)) +
        geom_point() +
        scale_color_manual(values = PColour, guide = FALSE) +
        stat_smooth(method = "lm", se = FALSE, color = "blue") +
        geom_segment(aes(x = 81, y = 0.77, xend = 81, yend = 1.935), colour = "red", linetype = "dashed") +
        theme_bw(base_size = 15)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Residuals}
  The red point is at (124, 3.44). Given that \texttt{SATSum} is 124, it is expected to be at 2.94. So, it's residual is $3.44 - 2.94 = 0.5$.
<<Res2, fig.height=4, echo=FALSE>>=
# Create Low Point
satGPASlim$HighPoint <- satGPASlim$FYGPA == 3.44 & satGPASlim$SATSum == 124

#Create plot of SATSum FYGPA 
ggplot(data = satGPASlim, aes(SATSum, FYGPA, color = HighPoint)) +
        geom_point() +
        scale_color_manual(values = PColour, guide = FALSE) +
        stat_smooth(method = "lm", se = FALSE, color = "blue") +
        geom_segment(aes(x = 124, y = 3.44, xend = 124, yend = 2.94), colour = "red", linetype = "dashed") +
        theme_bw(base_size = 15)
@
\end{frame}

\frame{
  \frametitle{Aim}
  We want to find the line that gets us the smallest {\bf{sum of squared residuals}} (SSR) where the SSR is:
  \[
    SSR = e^{2}_{1} + e^{2}_{2} + \ldots + e^{2}_{n}
  \]
}

\frame{
  \frametitle{Um\ldots}
  \begin{center}
{\LARGE{So where do the expected values ($\hat{y}$) come from?}}
  \end{center}
}

\frame{
  \frametitle{$\hat{\beta}$}
  Remember the simple linear regression equation:
  \[
  y = \alpha + \beta x
  \]
  We want to find the $\hat{\beta}$ that {\bf{minimizes}} the SSR. \\[0.5cm]
  Formally:
  \[
  \hat{\beta} = \argmin\limits_{b\in\mathbb{R}p}\mathrm{SSR}
  \]
  This is called {\bf{ordinary least squares simple linear regression.}}
}

\frame{
  \frametitle{How?}
  \begin{center}
    {\LARGE{How do we find the  $\hat{\beta}$ that {\bf{minimizes}} the SSR?}}
  \end{center}
}

\frame{
  \frametitle{By Hand}
  One way to estimate the correlation coefficient parameter $\beta$ for $x$ is with the following equation:
  \[
  \hat{\beta} = \frac{s_{y}}{s_{x}}R
  \]
}

\begin{frame}[fragile]
  \frametitle{Calculate $\hat{\beta}$}
<<CalcBeta>>=
# Find standard errors
SDy <- sd(satGPA$FYGPA)
SDx <- sd(satGPA$SATSum)

# Find correlation
CorXY <- cor(satGPA$SATSum, satGPA$FYGPA)

# Estiamte correlation coefficient
BetaHat <- (SDy/SDx) * CorXY

BetaHat
@
\end{frame}

\frame{
  \frametitle{Finding the intercept.}
  \begin{center}
{\LARGE{How can we find the intercept?}}
  \end{center}
}

\frame{
  \frametitle{Finding the intercept ($\hat{\alpha}$).}
  You might remember from maths that we can find the whole equation for a line if we know:
  \begin{itemize}
    \item<1-> a point on the line.
    \item<1-> the slope.
\end{itemize}\\[0.5cm]
  We know the slope and we know that the point at the mean of $x$ and $y$ ($\bar{x},\bar{y}$) will be on the line so we can use the equation for the {\bf{point-slope}} form of a line:
  \[
  y - \bar{y} = \hat{b}(x - \bar{x})
  \]
}

\frame{
  \frametitle{Finding the intercept ($\hat{\alpha}$).}
  If $\bar{y} = 2.468$, $\bar{x} = 103.329$, and $\hat{\beta} = 0.02387$, then:
  \[
  y - \bar{y} = \hat{\beta}(x - \bar{x})
  \]
  \[
  y - 2.468 = 0.02387(x - 103.329)
  \]
  \[
  y - 2.468 = 0.02387x - 2.466463
  \]
  \[
  y = 0.2387x + 0.001537
  \]
  \[
  \widehat{FYGPA} = 0.001537 + 0.2387SATSum
  \]
}


\frame{
  \frametitle{This class}
  \begin{center}
    {\Large{In this class we will let the computer find the $\hat{\beta}$ and $\hat{\alpha}$.}}
  \end{center}
}

\begin{frame}[fragile]
  \frametitle{Linear Model}
  In R you can use the \texttt{lm} (linear model) command. For example,
<<LM1>>=
M1 <- lm(FYGPA ~ SATSum, data = satGPA)

M1
@
\end{frame}

\frame{
  \frametitle{The Regression Equation}
  So again, our estimated regression equation is:
  \[
  \widehat{FYGPA} = 0.00193 + 0.02387SATSum
  \]
}

\frame{
  \frametitle{Assumptions}
{\LARGE{Linear Regression Assumptions:}} \\[0.5cm]
  \begin{itemize}
    \item The data follow a {\bf{linear trend}},
    \item Nearly {\bf{normally distributed residuals}},
    \item There is {\bf{constant variability}}.
  \end{itemize}
}

\frame{
  \frametitle{Example Assumption Violations}
  Which assumptions do these data violate?
  \begin{center}
    \includegraphics[scale=0.5]{AssumptionViolation.png}
  \end{center}
{\tiny{Source: Diaz et al. (2011, 285)}}
}

\begin{frame}[fragile,plain]
  {\small{To determine if the residuals in the model \texttt{M1} are normally distributed:}}
<<NormallyDistributed, fig.height=4>>=
# Find standardized residuals
M1.residuals <- rstandard(M1)
# Create Quantile-Quantile Plot
qqnorm(M1.residuals); qqline(M1.residuals)
@
\end{frame}

%%%%%%%%% Hypothesis testing with OLS
\section{Hypothesis Testing}
\frame{
  \frametitle{Hypothesis Testing}
  {\Large{Remember that $\hat{\beta}$ is a {\bf{point estimate}} of the {\bf{population parameter}} $\beta$.\\[0.5cm]
  How can we make {\bf{inferences}} about $\beta$ from $\hat{\beta}$?}}\\[1.5cm]
  Note: some people use $b$ to refer to $\hat{\beta}$.
}

\frame{
  \frametitle{Hypotheses}
  \begin{center}
{\LARGE{What would our null and alternative hypotheses be?}}
  \end{center}
}

\frame{
  \frametitle{Hypotheses}
  For our example, with a positive slope of 0.2387:\\[1cm]
{\large{$H_{0}$: $\beta = 0$ \\[0.5cm]
$H_{a}$: \beta > 0$}} 
}

\frame{
  \frametitle{P-values for $\hat{\beta}$}
  We usually assume that the sampling distribution of $\hat{\beta}$ follows a $t$ distribution. \\[0.25cm]
  Remember the equation for the $t\:test$ statistic:
  \[
  T = \frac{\mathrm{point\:estimate - null\:value}}{\mathrm{SE}}\\
  \]
  $\mathrm{with}\: n - 2\: \mathrm{degrees\:of\:freedom}$.\\[0.5cm]
  The procedure is the same as before to find the p-value.
}

\begin{frame}[fragile]
  \frametitle{Model Summary}
{\tiny{
<<M1Summary>>=
# Summarize M1 model output
summary(M1)
@
}}
\end{frame}

\frame{
  \frametitle{Inference \& p-values}
  So, we find evidence against the null hypothesis that the slope of the line summarizing the relationship between first year university grades and SAT total scores is 0 in the population. \\[0.5cm]
  Note: the p-vale given by \texttt{summary} is based on a {\bf{two-sided}} hypothesis. If we have a one sided hypothesis we can {\bf{halve}} the p-value. \\[0.5cm]
  In our example this would be impractical, since the p-value is already so small.
  
}

%%%%%%%%% Special Issues
\section{Some Special Issues in Simple Linear Regression}

\frame{
  \frametitle{Dummy Variables}
  So far we have only looked at creating simple linear regression models with {\bf{continuous numeric}} dependent and independent variables. \\[0.5cm]
  What if we have a {\bf{continuous dependent}} variable and a {\bf{dichotomous (dummy) independent}} variable?
}

\begin{frame}[fragile]
  \frametitle{Dichotomous Independent Variables}
  For example:
<<SexDichot1, echo=FALSE, fig.height=4>>=
# Recode Sex
satGPA$sex[satGPA$sex == 2] <- 0
satGPA$sex[satGPA$sex == 1] <- 1

ggplot(satGPA, aes(sex, FYGPA)) +
        geom_point() +
        scale_x_continuous(breaks = c(0, 1)) +
        theme_bw(base_size = 15)
@
{\tiny{Note, I recoded the values of the original variable.}}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Dichotomous Independent Variables}
  $\beta$ is pretty similar. It is still the slope of the line for a one unit change in $x$. The only difference is that the variable only goes between 0 and 1.
<<SexDichot2, echo=FALSE, fig.height=4>>=
# Recode Sex
satGPA$sex[satGPA$sex == 2] <- 0
satGPA$sex[satGPA$sex == 1] <- 1

ggplot(satGPA, aes(sex, FYGPA)) +
        geom_point() +
        geom_smooth(method = "lm", se = FALSE) +
        scale_x_continuous(breaks = c(0, 1)) +
        theme_bw(base_size = 15)
@
{\tiny{Note, I recoded the values of the original variable.}}
\end{frame}

\frame{
  \frametitle{Categorical Dependent}
  What if our {\bf{dependent variable}} is categorical, for example, the party someone voted for? \\[0.5cm]
  For these situations you need to use a different type of regression, for example {\bf{logistic regression}}. \\[0.5cm]
  We do not cover this type of regression in this course.
}

\frame{
  \frametitle{Extrapolation}
{\Large{Be careful about {\bf{extrapolating}} beyond your data. \\[0.5cm]
  We don't know how the data beyond what we observe will behave.}}
}

\begin{frame}[fragile]
  \frametitle{Extrapolation}
  On the 37th day of the year it was 10 degrees.
<<Extrap1, echo=FALSE, fig.height=4>>=
Day <- 37
Temp <- 10
Data <- data.frame(Day, Temp)

ggplot(Data, aes(Day, Temp)) +
        geom_point(size = 7) +
        scale_x_continuous(limits = c(1, 365), 
                           breaks = c(1, 37, 100, 200, 300, 365), 
                           labels = c(1, 37, 100, 200, 300, 365)) +
        scale_y_continuous(limits = c(-5, 30)) +
        theme_bw(base_size = 15)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Extrapolation}
  On the 70th day of the year it was 20 degrees.
<<Extrap2, echo=FALSE, fig.height=4>>=
Day <- c(37, 70)
Temp <- c(10, 20)
Data <- data.frame(Day, Temp)

ggplot(Data, aes(Day, Temp)) +
        geom_point(size = 7) +
        scale_x_continuous(limits = c(1, 365), 
                           breaks = c(1, 37, 70, 100, 200, 300, 365), 
                           labels = c(1, 37, 70, 100, 200, 300, 365)) +
        scale_y_continuous(limits = c(-5, 30)) +
        theme_bw(base_size = 15)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Extrapolation}
  So on the last day of the year it will be about 100 degrees?.
<<Extrap3, echo=FALSE, fig.height=4>>=
Day <- c(37, 70, 365)
Temp <- c(10, 20, 100)
Data <- data.frame(Day, Temp)

ggplot(Data, aes(Day, Temp)) +
        geom_point(size = 7) +
        geom_smooth(method = "lm", se = FALSE) +
        scale_x_continuous(limits = c(1, 365), 
                           breaks = c(1, 37, 70, 100, 200, 300, 365), 
                           labels = c(1, 37, 70, 100, 200, 300, 365)) +
        scale_y_continuous(limits = c(-5,105)) +
        theme_bw(base_size = 15)
@
\end{frame}

\frame{
  \frametitle{Outliers}
  Be careful about outliers.
  \begin{center}
    \includegraphics[scale=0.3]{Outliers.png}
  \end{center}\\[0.2cm]
{\tiny{Source: Diaz et al. (2011, 290)}}
}

\frame{
  \frametitle{Removing Outliers}
{\Large{Only remove outliers if you have a {\bf{good reason}} to. \\[0.5cm]
        Try to find out {\bf{substantively}} why they are outliers.}}
}




\begin{frame}[allowframebreaks]
  \frametitle{References}
  Crawley, Michael J. 2005. Statistics: An Introduction Using R. Chichester: John Wiley & Sons. Ltd. \\[0.25cm]
  Diaz, David M., Christopher D. Barr, and Mine \c{C}etinkaya-Rundel. 2011. OpenIntro Statistics. 1st ed. \url{http://www.openintro.org/stat/downloads.php}. \\[0.25cm] 
\end{frame}


\end{document}