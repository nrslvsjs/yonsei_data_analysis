\documentclass{beamer}\usepackage{graphicx, color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\definecolor{fgcolor}{rgb}{0.2, 0.2, 0.2}
\newcommand{\hlnumber}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlfunctioncall}[1]{\textcolor[rgb]{0.501960784313725,0,0.329411764705882}{\textbf{#1}}}%
\newcommand{\hlstring}[1]{\textcolor[rgb]{0.6,0.6,1}{#1}}%
\newcommand{\hlkeyword}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlargument}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlcomment}[1]{\textcolor[rgb]{0.180392156862745,0.6,0.341176470588235}{#1}}%
\newcommand{\hlroxygencomment}[1]{\textcolor[rgb]{0.43921568627451,0.47843137254902,0.701960784313725}{#1}}%
\newcommand{\hlformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hleqformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlassignement}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlpackage}[1]{\textcolor[rgb]{0.588235294117647,0.709803921568627,0.145098039215686}{#1}}%
\newcommand{\hlslot}[1]{\textit{#1}}%
\newcommand{\hlsymbol}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlprompt}[1]{\textcolor[rgb]{0.2,0.2,0.2}{#1}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usetheme{Stats}
\setbeamercovered{transparent}
\usepackage{color}
\usepackage{hyperref}
  \hypersetup{
  	colorlinks=true
		linkcolor=black
		}
\usepackage{url}
\usepackage{graphics}
\usepackage{tikz}
\usepackage{booktabs}

\DeclareMathOperator*{\argmin}{arg\,min}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title Slide %%%%%%%%%%%%%%%%%%%%%%%%%%
\title[]{Intro to Social Science Data Analysis \\[1cm] Week 11 Lecture: Simple Linear Regression}
\author[]{
    \href{mailto:gandrud@yonsei.ac.kr}{Christopher Gandrud}
}
\date{\today}


\begin{document}

\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents}

\frame{
  \frametitle{Assignment 3}
  {\Large{Like all data assignments in this course, your response to Assignment 3 must be \textbf{reproducible}.}}
}

\section{Recap}
\frame{
	\frametitle{Quick Quiz 1}
  Find the sample proportions of the following party's supporters:
  \begin{table}
    \begin{tabular}{c c c c}
    \hline
    Saenuri & DUP & Other & Total \\
    \hline\hline
    1064 & 891 & 520 & 2475 \\ 
    \hline
    \end{tabular}
  \end{table}
}

\frame{
  \frametitle{Quick Quiz 1}
  \begin{table}
    \begin{tabular}{c c c c}
    \hline
    Saenuri & DUP & Other & Total \\
    \hline\hline
    1064 & 891 & 520 & 2475 \\ 
    (0.43) & (0.36) & (0.21) & (1) \\
    \hline
    \end{tabular}
  \end{table}
}

\frame{
  \frametitle{Quick Quiz 2}
  If we wanted to make inferences about {\bf{population proportions}} from sampling proportions, what {\bf{distribution}} do we often assume the sampling proportions follow? \\[0.5cm]
  
  What are it's {\bf{parameters}}?
}

\frame{
  \frametitle{Quick Quix 3}
  Imagine we have a two-way contingency table.
  \begin{table}
    \begin{tabular}{c c c c}
    \hline
     & Attend University & No University \\
    \hline\hline
    Married &&\\ 
    Not Married && \\
    \hline
    \end{tabular}
  \end{table}
  If we conducted a $\chi^{2}$ test with this data and found a p-value of $<0.001$ what would we conclude?
}

\frame{
  \frametitle{Quick Quiz 4}
  Write the simple linear regression equation for how a person's height is related to their income.
}

\frame{
  \frametitle{Quick Quiz 5}
  Describe how a linear regression line would look if the relationship between two variables was negative. \\[0.5cm]
  How would it look if the relationship was positive? \\[0.5cm]
  What about no relationship?
}

\frame{
  \frametitle{Quick Quiz 6}
  Describe the variables in the simple linear regression equation.
  \[
  y = \alpha + \beta x
  \]
}

%%%%%%%%%%%% Correlation
\section{Correlation}
\frame{
  \framtitle{Motivation}
  Since almost no interesting relationship is perfectly linear, how do we find the {\bf{best fit line}} that describes the relationship between some $x$ and some $y$?
}

\frame{
  \frametitle{How?}
  \begin{center}
    \includegraphics[scale=0.5]{/git_repositories/Introduction_to_Statistics_and_Data_Analysis_Yonsei/Lectures/Lecture10/ExampleSLR.png} \\[0.5cm]
  \end{center}  
{\tiny{Source: Diaz et. al. (2011, 216)}}
}

\frame{
  In {\bf{simple linear regression}} we are trying to find the straight line that is {\bf{as close to all of the data points as possible}}.\\[1cm]
  How do we find this line?
}

\begin{frame}[fragile]
  Let's use the SAT/GPA data from the \texttt{openintro} package:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcomment{# Load library}
\hlfunctioncall{library}(openintro)

\hlcomment{# Load data}
\hlfunctioncall{data}(satGPA)

\hlcomment{# Show variables}
\hlfunctioncall{names}(satGPA)
\end{alltt}
\begin{verbatim}
## [1] "sex"    "SATV"   "SATM"   "SATSum" "HSGPA" 
## [6] "FYGPA"
\end{verbatim}
\begin{alltt}

\hlcomment{# Subset to remove the sex variable}
satGPASlim <- satGPA[, 2:6]
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Plot the SAT Scores \& GPAs}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{library}(GGally)

\hlfunctioncall{ggpairs}(satGPASlim, upper = \hlstring{"blank"})
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{figure/DescribeGPA} 

}


\end{knitrout}

\end{frame}

\begin{frame}[fragile]
  \frametitle{First Year GPA}
  Universities want to know how well student's total SAT scores (\texttt{SATSum}) relate to their academic performance in the first year of university (\texttt{FYGPA}).
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/SATFYGPA} 

}


\end{knitrout}

\end{frame}

\frame{
  \frametitle{Correlation}
  One way to describe the overall relationship between \texttt{SATSum} and \texttt{FYGPA} is to find the {\bf{correlation}} between the two variables. 
}

\frame{
  \frametitle{Correlation}
{\LARGE{Correlation ($R$):}} \\[0.5cm]
  Describes the {\bf{strength}} of a linear relationship. \\[0.25cm]
  It ranges from -1 to 1. \\[0.5cm]
  -1 indicates a {\bf{perfect negative relationship}}.\\[0.25cm]
  1 indicates a {\bf{perfect positive relationship}}. \\[0.25cm]
  0 indicates {\bf{no correlation/relationship}}.  
}

\frame{
  \frametitle{Correlation}
  To find the correlation for observations $(x_{1}, y_{1}),\:(x_{2}, y_{2})\ldots(x_{n}, y_{n})$
\[
  R = \frac{1}{n - 1}\sum\limits_{i = 1}^n\frac{x_{i} - \bar{x}}{s_{x}}\frac{y_{i} - \bar{y}}{s_{y}}
\] 
}

\begin{frame}[fragile]
  \frametitle{Or\ldots}
  Or we can have R do the maths for us.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{cor}(satGPA$SATSum, satGPA$FYGPA)
\end{alltt}
\begin{verbatim}
## [1] 0.4603
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\frame{
  \frametitle{Statistical Significance \& Correlation}
  If we wanted to test to see if the correlation is statistically significant, what would the null hypothesis be?
}

\frame{
  \frametitle{Statistical Significance \& Correlation}
{\large{$H_{0}$: $R = 0$ \\[0.5cm]
$H_{a}$: R \neq 0$}} 
}

\begin{frame}[fragile]
  \frametitle{Hypothesis Testing Correlation Coefficients in R}
{\scriptsize{
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{cor.test}(satGPA$SATSum, satGPA$FYGPA)
\end{alltt}
\begin{verbatim}
## 
## 	Pearson's product-moment correlation
## 
## data:  satGPA$SATSum and satGPA$FYGPA 
## t = 16.38, df = 998, p-value < 2.2e-16
## alternative hypothesis: true correlation is not equal to 0 
## 95 percent confidence interval:
##  0.4100 0.5078 
## sample estimates:
##    cor 
## 0.4603
\end{verbatim}
\end{kframe}
\end{knitrout}

}}
\end{frame}

\frame{
  \frametitle{More Correlation Examples}
  \begin{center}
  \includegraphics[scale=0.45]{LinearCor.png} \\[0.5cm]
  \end{center}
{\tiny{Source: Diaz et al. (2011, 282 )}}
}

\frame{
  \frametitle{Caution}
  A low linear correlation {\bf{does not necessarily}} mean a weak relationship. \\[0.5cm]
  It means a weak {\bf{linear}} relationship.
  \begin{center}
  \includegraphics[scale=0.45]{NonLinearCor.png} \\[0.5cm]
  \end{center}
{\tiny{Source: Diaz et al. (2011, 282 )}}
}

%%%%%%%%%%%% Least Squares Regression
\section{Best Fit Lines \& Least Squares Regression}
\frame{
  \frametitle{Best Fit Lines \& Least Squares Regression}
  Ok, linear correlations are useful for finding:
  \begin{itemize}
    \item the {\bf{direction}} of a linear relationship,
    \item the {\bf{strength}} of a linear relationship.
  \end{itemize}
}

\frame{
  \frametitle{More specific}
  What if we want to be more specific? \\[0.5cm]
  For example, using a student's total SAT score to predict their first year university GPA.\\[0.5cm]
{\bf{Note:}} the estimated value of the dependent variable ($y$) is often written $\hat{y}$ ({\emph{``y hat"}}).

}

\begin{frame}[fragile]
  \frametitle{The Linear Best Fit Line}
  The blue line is the closest straight line (``best fit") to all of the data points.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/BestFit} 

}


\end{knitrout}

\end{frame}

\frame{
  \frametitle{How?}
  \begin{center}
    {\LARGE{How do we find the best fit line?}}
  \end{center}
}

\frame{
  \frametitle{Residuals}
  Well, the best fit line would do something like have the smallest {\bf{residuals}} possible. \\[0.5cm]
  What is a residual?
}

\frame{
  \frametitle{Residuals}
{\LARGE{Residual:}} \\[0.5cm]
  the difference between the observed and expected values based on the best fit model.\\[0.5cm]
  More formally: the residual ($e_{i}$) of the observation $(x_{i}, y_{i})$ is the difference between the observed value of $y_{i}$ and the expected value $\hat{y}_{i}$:
  \[
  e_{i} = y_{i} - \hat{y}_{i}
  \]
}

\begin{frame}[fragile]
  \frametitle{Residuals}
  The red point is at (81, 0.77). Given that \texttt{SATSum} is 81, it is expected to be at 1.935. So, it's residual is $0.77 - 1.935 = -1.65$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/Res1} 

}


\end{knitrout}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Residuals}
  The red point is at (124, 3.44). Given that \texttt{SATSum} is 124, it is expected to be at 2.94. So, it's residual is $3.44 - 2.94 = 0.5$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/Res2} 

}


\end{knitrout}

\end{frame}

\frame{
  \frametitle{Aim}
  We want to find the line that gets us the smallest {\bf{sum of squared residuals}} (SSR) where the SSR is:
  \[
    SSR = e^{2}_{1} + e^{2}_{2} + \ldots + e^{2}_{n}
  \]
}

\frame{
  \frametitle{Um\ldots}
  \begin{center}
{\LARGE{So where do the expected values ($\hat{y}$) come from?}}
  \end{center}
}

\frame{
  \frametitle{$\hat{\beta}$}
  Remember the simple linear regression equation:
  \[
  y = \alpha + \beta x
  \]
  We want to find the $\hat{\beta}$ that {\bf{minimizes}} the SSR. \\[0.5cm]
  Formally:
  \[
  \hat{\beta} = \argmin\limits_{b\in\mathbb{R}p}\mathrm{SSR}
  \]
  This is called {\bf{ordinary least squares simple linear regression.}}
}

\frame{
  \frametitle{How?}
  \begin{center}
    {\LARGE{How do we find the  $\hat{\beta}$ that {\bf{minimizes}} the SSR?}}
  \end{center}
}

\frame{
  \frametitle{By Hand}
  One way to estimate the correlation coefficient parameter $\beta$ for $x$ is with the following equation:
  \[
  \hat{\beta} = \frac{s_{y}}{s_{x}}R
  \]
}

\begin{frame}[fragile]
  \frametitle{Calculate $\hat{\beta}$}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcomment{# Find standard errors}
SDy <- \hlfunctioncall{sd}(satGPA$FYGPA)
SDx <- \hlfunctioncall{sd}(satGPA$SATSum)

\hlcomment{# Find correlation}
CorXY <- \hlfunctioncall{cor}(satGPA$SATSum, satGPA$FYGPA)

\hlcomment{# Estiamte correlation coefficient}
BetaHat <- (SDy/SDx) * CorXY

BetaHat
\end{alltt}
\begin{verbatim}
## [1] 0.02387
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\frame{
  \frametitle{Finding the intercept.}
  \begin{center}
{\LARGE{How can we find the intercept?}}
  \end{center}
}

\frame{
  \frametitle{Finding the intercept ($\hat{\alpha}$).}
  You might remember from maths that we can find the whole equation for a line if we know:
  \begin{itemize}
    \item<1-> a point on the line.
    \item<1-> the slope.
\end{itemize}\\[0.5cm]
  We know the slope and we know that the point at the mean of $x$ and $y$ ($\bar{x},\bar{y}$) will be on the line so we can use the equation for the {\bf{point-slope}} form of a line:
  \[
  y - \bar{y} = \hat{b}(x - \bar{x})
  \]
}

\frame{
  \frametitle{Finding the intercept ($\hat{\alpha}$).}
  If $\bar{y} = 2.468$, $\bar{x} = 103.329$, and $\hat{\beta} = 0.02387$, then:
  \[
  y - \bar{y} = \hat{\beta}(x - \bar{x})
  \]
  \[
  y - 2.468 = 0.02387(x - 103.329)
  \]
  \[
  y - 2.468 = 0.02387x - 2.466463
  \]
  \[
  y = 0.2387x + 0.001537
  \]
  \[
  \widehat{FYGPA} = 0.001537 + 0.2387SATSum
  \]
}


\frame{
  \frametitle{This class}
  \begin{center}
    {\Large{In this class we will let the computer find the $\hat{\beta}$ and $\hat{\alpha}$.}}
  \end{center}
}

\begin{frame}[fragile]
  \frametitle{Linear Model}
  In R you can use the \texttt{lm} (linear model) command. For example,
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
M1 <- \hlfunctioncall{lm}(FYGPA ~ SATSum, data = satGPA)

M1
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = FYGPA ~ SATSum, data = satGPA)
## 
## Coefficients:
## (Intercept)       SATSum  
##     0.00193      0.02387
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\frame{
  \frametitle{The Regression Equation}
  So again, our estimated regression equation is:
  \[
  \widehat{FYGPA} = 0.00193 + 0.02387SATSum
  \]
}

\frame{
  \frametitle{Assumptions}
{\LARGE{Linear Regression Assumptions:}} \\[0.5cm]
  \begin{itemize}
    \item The data follow a {\bf{linear trend}},
    \item Nearly {\bf{normally distributed residuals}},
    \item There is {\bf{constant variability}}.
  \end{itemize}
}

\frame{
  \frametitle{Example Assumption Violations}
  Which assumptions do these data violate?
  \begin{center}
    \includegraphics[scale=0.5]{AssumptionViolation.png}
  \end{center}
{\tiny{Source: Diaz et al. (2011, 285)}}
}

\begin{frame}[fragile,plain]
  {\small{To determine if the residuals in the model \texttt{M1} are normally distributed:}}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcomment{# Find standardized residuals}
M1.residuals <- \hlfunctioncall{rstandard}(M1)
\hlcomment{# Create Quantile-Quantile Plot}
\hlfunctioncall{qqnorm}(M1.residuals)
\hlfunctioncall{qqline}(M1.residuals)
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{figure/NormallyDistributed} 

}


\end{knitrout}

\end{frame}

%%%%%%%%% Hypothesis testing with OLS
\section{Hypothesis Testing}
\frame{
  \frametitle{Hypothesis Testing}
  {\Large{Remember that $\hat{\beta}$ is a {\bf{point estimate}} of the {\bf{population parameter}} $\beta$.\\[0.5cm]
  How can we make {\bf{inferences}} about $\beta$ from $\hat{\beta}$?}}\\[1.5cm]
  Note: some people use $b$ to refer to $\hat{\beta}$.
}

\frame{
  \frametitle{Hypotheses}
  \begin{center}
{\LARGE{What would our null and alternative hypotheses be?}}
  \end{center}
}

\frame{
  \frametitle{Hypotheses}
  For our example, with a positive slope of 0.2387:\\[1cm]
{\large{$H_{0}$: $\beta = 0$ \\[0.5cm]
$H_{a}$: \beta > 0$}} 
}

\frame{
  \frametitle{P-values for $\hat{\beta}$}
  We usually assume that the sampling distribution of $\hat{\beta}$ follows a $t$ distribution. \\[0.25cm]
  Remember the equation for the $t\:test$ statistic:
  \[
  T = \frac{\mathrm{point\:estimate - null\:value}}{\mathrm{SE}}\\
  \]
  $\mathrm{with}\: n - 2\: \mathrm{degrees\:of\:freedom}$.\\[0.5cm]
  The procedure is the same as before to find the p-value.
}

\begin{frame}[fragile]
  \frametitle{Model Summary}
{\tiny{
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcomment{# Summarize M1 model output}
\hlfunctioncall{summary}(M1)
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = FYGPA ~ SATSum, data = satGPA)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.1976 -0.4495  0.0315  0.4557  1.6115 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)  0.00193    0.15199    0.01     0.99
## SATSum       0.02387    0.00146   16.38   <2e-16
##                
## (Intercept)    
## SATSum      ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Residual standard error: 0.658 on 998 degrees of freedom
## Multiple R-squared: 0.212,	Adjusted R-squared: 0.211 
## F-statistic:  268 on 1 and 998 DF,  p-value: <2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}

}}
\end{frame}

\frame{
  \frametitle{Inference \& p-values}
  So, we find evidence against the null hypothesis that the slope of the line summarizing the relationship between first year university grades and SAT total scores is 0 in the population. \\[0.5cm]
  Note: the p-vale given by \texttt{summary} is based on a {\bf{two-sided}} hypothesis. If we have a one sided hypothesis we can {\bf{halve}} the p-value. \\[0.5cm]
  In our example this would be impractical, since the p-value is already so small.
  
}

%%%%%%%%% Special Issues
\section{Some Special Issues in Simple Linear Regression}

\frame{
  \frametitle{Dummy Variables}
  So far we have only looked at creating simple linear regression models with {\bf{continuous numeric}} dependent and independent variables. \\[0.5cm]
  What if we have a {\bf{continuous dependent}} variable and a {\bf{dichotomous (dummy) independent}} variable?
}

\begin{frame}[fragile]
  \frametitle{Dichotomous Independent Variables}
  For example:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/SexDichot1} 

}


\end{knitrout}

{\tiny{Note, I recoded the values of the original variable.}}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Dichotomous Independent Variables}
  $\beta$ is pretty similar. It is still the slope of the line for a one unit change in $x$. The only difference is that the variable only goes between 0 and 1.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/SexDichot2} 

}


\end{knitrout}

{\tiny{Note, I recoded the values of the original variable.}}
\end{frame}

\frame{
  \frametitle{Categorical Dependent}
  What if our {\bf{dependent variable}} is categorical, for example, the party someone voted for? \\[0.5cm]
  For these situations you need to use a different type of regression, for example {\bf{logistic regression}}. \\[0.5cm]
  We do not cover this type of regression in this course.
}

\frame{
  \frametitle{Extrapolation}
{\Large{Be careful about {\bf{extrapolating}} beyond your data. \\[0.5cm]
  We don't know how the data beyond what we observe will behave.}}
}

\begin{frame}[fragile]
  \frametitle{Extrapolation}
  On the 37th day of the year it was 10 degrees.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/Extrap1} 

}


\end{knitrout}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Extrapolation}
  On the 70th day of the year it was 20 degrees.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/Extrap2} 

}


\end{knitrout}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Extrapolation}
  So on the last day of the year it will be about 100 degrees?.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/Extrap3} 

}


\end{knitrout}

\end{frame}

\frame{
  \frametitle{Outliers}
  Be careful about outliers.
  \begin{center}
    \includegraphics[scale=0.3]{Outliers.png}
  \end{center}\\[0.2cm]
{\tiny{Source: Diaz et al. (2011, 290)}}
}

\frame{
  \frametitle{Removing Outliers}
{\Large{Only remove outliers if you have a {\bf{good reason}} to. \\[0.5cm]
        Try to find out {\bf{substantively}} why they are outliers.}}
}




\begin{frame}[allowframebreaks]
  \frametitle{References}
  Crawley, Michael J. 2005. Statistics: An Introduction Using R. Chichester: John Wiley & Sons. Ltd. \\[0.25cm]
  Diaz, David M., Christopher D. Barr, and Mine \c{C}etinkaya-Rundel. 2011. OpenIntro Statistics. 1st ed. \url{http://www.openintro.org/stat/downloads.php}. \\[0.25cm] 
\end{frame}


\end{document}
